{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d77c9a2",
   "metadata": {},
   "source": [
    "# Batch Project for Data Engineering Pipeline Using AWS\n",
    "\n",
    "This is a project done by Albert Lok to practice creating a data pipeline. The project covers these topics:\n",
    "\n",
    "Credit to @josephmachado for the project design and code.\n",
    "\n"
    "1. Set up Apache Airflow, AWS EMR, AWS Redshift, AWS Spectrum, and AWS S3.\n",
    "\n",
    "2. Learn data pipeline best practices.\n",
    "\n",
    "3. Learn how to spot failure points in data pipelines and build systems resistant to failures.\n",
    "\n",
    "4. Learn how to design and build a data pipeline from business requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7134e1",
   "metadata": {},
   "source": [
    "## OBJECTIVE\n",
    "\n",
    "The scenario is a user behavior analytics company that collects user data and creates a user profile. We are tasked with building a data pipeline to populate the `user_behavior_metric` table. The `user_behavior_metric` table is an OLAP table, meant to be used by analysts, dashboard software, etc. It is built from\n",
    "\n",
    "1. `user_purchase`: OLTP table with user purchase information.\n",
    "2. `movie_review.csv`: Data sent every day by an external data vendor.\n",
    "\n",
    "![Entity Relationship Diagram](https://www.startdataengineering.com/images/de_project_for_beginners/de_proj_obj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e84f20",
   "metadata": {},
   "source": [
    "## DESIGN\n",
    "\n",
    "We used Airflow to orchestrate the following tasks:\n",
    "\n",
    "1. Classifying movie reviews with Apache Spark.\n",
    "2. Loading the classified movie reviews into the data warehouse.\n",
    "3. Extracting user purchase data from an OLTP database and loading it into the data warehouse.\n",
    "4. Joining the classified movie review data and user purchase data to get `user behavior metric` data.\n",
    "5. Visualized the data using Metabase\n",
    "\n",
    "![Design Diagram](https://www.startdataengineering.com/images/de_project_for_beginners/de_proj_design.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97785999",
   "metadata": {},
   "source": [
    "## SETUP\n",
    "\n",
    "To set up the AWS infrastructure, we set up and configured AWS CLI.  We have a script called setup_infra.sh that contains all the commands to set up on AWS us-east-1.  The setup script was run line by line to:\n",
    "\n",
    "* create an EC2 instance\n",
    "* create an S3 bucket\n",
    "* create an IAM role for EC2 and S3 access\n",
    "* attach policies to the IAM role\n",
    "* create an ssh key to connect to the EC2 instance\n",
    "* create an EC2 security group and modify inbound and outbound rules\n",
    "* copy code from local to EC2 server\n",
    "* create an EMR cluster\n",
    "* create a Redshift cluster\n",
    "* create tables on Redshift\n",
    "* spin up an Airflow Docker container\n",
    "* connect Redshift, postgres, S3, and EMR to Airflow\n",
    "* create log files\n",
    "* set up connections to Airflow UI and Metabase\n",
    "\n",
    "AWS variables used were stored in infra_variables.txt.\n",
    "\n",
    "Using AWS S3 as our data lake, data from external systems were stored here for further processing. AWS S3 was used as storage for use with AWS Redshift Spectrum.\n",
    "\n",
    "\n",
    "In our project, we used one bucket with multiple folders:\n",
    "* raw - to store raw data\n",
    "* stage - to stage data for processing\n",
    "* scripts - to store Spark script, for use by AWS EMR\n",
    "\n",
    "\n",
    "The script created these tables in Redshift:\n",
    "1. `retail.user_purchase` table, defined at `pgsetup/create_user_purchase.sql` in the repository. The data is mounted into the Postgres container’s file system. This data is loaded into the table using the COPY command.\n",
    "2. `spectrum.user_purchase_staging` table, defined as having its data stored in the data lake’s stage location. Note that the table also has a partition defined on the insert_date.\n",
    "3. `spectrum.classified_movie_review` table, defined as having its data stored in the data lake’s stage location.\n",
    "4. `public.user_behavior_metric` table is the table that we want to load data into.\n",
    "\n",
    "The script also created these Airflow connections and variables:\n",
    "* redshift connection: To connect to the AWS Redshift cluster.\n",
    "* postgres_default connection: To connect to the local Postgres database.\n",
    "* BUCKET variable: To indicate the bucket to be used as the data lake for this pipeline.\n",
    "* EMR_ID variable: To send commands to an AWS EMR cluster.\n",
    "\n",
    "![Redshift Connections](https://www.startdataengineering.com/images/de_project_for_beginners/admin_conn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8724c6eb",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "The data for `user_behavior_metric` is generated from 2 main datasets.\n",
    "\n",
    "### Loading user purchase data into the data warehouse\n",
    "To load the user purchase data from Postgres into AWS Redshift we run the following tasks:\n",
    "\n",
    "1. extract_user_purchase_data: Unloads data from Postgres to a local file system in the Postgres container. This filesystem is volume synced between our local Postgres and Airflow containers. This allows Airflow to access this data.\n",
    "2. user_purchase_to_stage_data_lake: Moves the extracted data to data lake’s staging area at `stage/user_purchase/{{ ds }}/user_purchase.csv`, where ds will be replaced by the run date in `YYYY-MM-DD` format. This `ds` will serve as the `insert_date` partition, defined at table creation.\n",
    "3. user_purchase_stage_data_lake_to_stage_tbl: Runs a Redshift query to make the `spectrum.user_purchase_staging` table aware of the new date partition.\n",
    "\n",
    "```\n",
    "extract_user_purchase_data = PostgresOperator(\n",
    "    dag=dag,\n",
    "    task_id=\"extract_user_purchase_data\",\n",
    "    sql=\"./scripts/sql/unload_user_purchase.sql\",\n",
    "    postgres_conn_id=\"postgres_default\",\n",
    "    params={\"user_purchase\": \"/temp/user_purchase.csv\"},\n",
    "    depends_on_past=True,\n",
    "    wait_for_downstream=True,\n",
    ")\n",
    "\n",
    "user_purchase_to_stage_data_lake = PythonOperator(\n",
    "    dag=dag,\n",
    "    task_id=\"user_purchase_to_stage_data_lake\",\n",
    "    python_callable=_local_to_s3,\n",
    "    op_kwargs={\n",
    "        \"file_name\": \"/temp/user_purchase.csv\",\n",
    "        \"key\": \"stage/user_purchase/{{ ds }}/user_purchase.csv\",\n",
    "        \"bucket_name\": BUCKET_NAME,\n",
    "        \"remove_local\": \"true\",\n",
    "    },\n",
    ")\n",
    "\n",
    "user_purchase_stage_data_lake_to_stage_tbl = PythonOperator(\n",
    "    dag=dag,\n",
    "    task_id=\"user_purchase_stage_data_lake_to_stage_tbl\",\n",
    "    python_callable=run_redshift_external_query,\n",
    "    op_kwargs={\n",
    "        \"qry\": \"alter table spectrum.user_purchase_staging add if not exists partition(insert_date='{{ ds }}') \\\n",
    "            location 's3://\"\n",
    "        + BUCKET_NAME\n",
    "        + \"/stage/user_purchase/{{ ds }}'\",\n",
    "    },\n",
    ")\n",
    "\n",
    "extract_user_purchase_data >> user_purchase_to_stage_data_lake >> user_purchase_stage_data_lake_to_stage_tbl\n",
    "```\n",
    "\n",
    "SQL:\n",
    "```\n",
    "COPY (\n",
    "       select invoice_number,\n",
    "              stock_code,\n",
    "              detail,\n",
    "              quantity,\n",
    "              invoice_date,\n",
    "              unit_price,\n",
    "              customer_id,\n",
    "              country\n",
    "       from retail.user_purchase -- we should have a date filter here to pull only required date's data\n",
    ") TO '{{ params.user_purchase }}' WITH (FORMAT CSV, HEADER);\n",
    "-- user_purchase will be replaced with /temp/user_purchase.csv from the params in extract_user_purchase_data task\n",
    "```\n",
    "\n",
    "### Loading classified movie review data into the data warehouse\n",
    "\n",
    "To get the classified movie review data into AWS Redshift, we run the following tasks:\n",
    "\n",
    "1. movie_review_to_raw_data_lake: Copies local file `data/movie_review.csv` to data lake’s raw area.\n",
    "2. spark_script_to_s3: Copies our pyspark script to data lake’s script area. This allows AWS EMR to reference it.\n",
    "3. start_emr_movie_classification_script: Adds the EMR steps defined at `dags/scripts/emr/clean_movie_review.json` to our EMR cluster. This task adds 3 EMR steps to the cluster, they do the following:\n",
    "* Moves raw data from S3 to HDFS: Copies data from data lake’s raw area into EMR’s HDFS.\n",
    "* Classifies movie reviews: Runs the review classification pyspark script.\n",
    "* Moves classified data from HDFS to S3: Copies data from EMR’s HDFS to data lake’s staging area.\n",
    "4. wait_for_movie_classification_transformation: This is a sensor task that waits for the final step (`Move classified data from HDFS to S3`) to finish.\n",
    "\n",
    "```\n",
    "movie_review_to_raw_data_lake = PythonOperator(\n",
    "    dag=dag,\n",
    "    task_id=\"movie_review_to_raw_data_lake\",\n",
    "    python_callable=_local_to_s3,\n",
    "    op_kwargs={\n",
    "        \"file_name\": \"/data/movie_review.csv\",\n",
    "        \"key\": \"raw/movie_review/{{ ds }}/movie.csv\",\n",
    "        \"bucket_name\": BUCKET_NAME,\n",
    "    },\n",
    ")\n",
    "\n",
    "spark_script_to_s3 = PythonOperator(\n",
    "    dag=dag,\n",
    "    task_id=\"spark_script_to_s3\",\n",
    "    python_callable=_local_to_s3,\n",
    "    op_kwargs={\n",
    "        \"file_name\": \"./dags/scripts/spark/random_text_classification.py\",\n",
    "        \"key\": \"scripts/random_text_classification.py\",\n",
    "        \"bucket_name\": BUCKET_NAME,\n",
    "    },\n",
    ")\n",
    "\n",
    "start_emr_movie_classification_script = EmrAddStepsOperator(\n",
    "    dag=dag,\n",
    "    task_id=\"start_emr_movie_classification_script\",\n",
    "    job_flow_id=EMR_ID,\n",
    "    aws_conn_id=\"aws_default\",\n",
    "    steps=EMR_STEPS,\n",
    "    params={\n",
    "        \"BUCKET_NAME\": BUCKET_NAME,\n",
    "        \"raw_movie_review\": \"raw/movie_review\",\n",
    "        \"text_classifier_script\": \"scripts/random_text_classifier.py\",\n",
    "        \"stage_movie_review\": \"stage/movie_review\",\n",
    "    },\n",
    "    depends_on_past=True,\n",
    ")\n",
    "\n",
    "last_step = len(EMR_STEPS) - 1\n",
    "\n",
    "wait_for_movie_classification_transformation = EmrStepSensor(\n",
    "    dag=dag,\n",
    "    task_id=\"wait_for_movie_classification_transformation\",\n",
    "    job_flow_id=EMR_ID,\n",
    "    step_id='{{ task_instance.xcom_pull(\"start_emr_movie_classification_script\", key=\"return_value\")['\n",
    "    + str(last_step)\n",
    "    + \"] }}\",\n",
    "    depends_on_past=True,\n",
    ")\n",
    "\n",
    "[\n",
    "    movie_review_to_raw_data_lake,\n",
    "    spark_script_to_s3,\n",
    "] >> start_emr_movie_classification_script >> wait_for_movie_classification_transformation\n",
    "```\n",
    "\n",
    "### Generating user behavior metric\n",
    "\n",
    "With both the `user purchase` data and the `classified movie` data in the data warehouse, we can get the data for the `user_behavior_metric` table. This is done using the `generate_user_behavior_metric` task. This task runs a redshift SQL script to populate the `public.user_behavior_metric` table.\n",
    "\n",
    "```\n",
    "generate_user_behavior_metric = PostgresOperator(\n",
    "    dag=dag,\n",
    "    task_id=\"generate_user_behavior_metric\",\n",
    "    sql=\"scripts/sql/generate_user_behavior_metric.sql\",\n",
    "    postgres_conn_id=\"redshift\",\n",
    ")\n",
    "\n",
    "end_of_data_pipeline = DummyOperator(task_id=\"end_of_data_pipeline\", dag=dag) # dummy operator to indicate DAG complete\n",
    "\n",
    "[\n",
    "    user_purchase_stage_data_lake_to_stage_tbl,\n",
    "    wait_for_movie_classification_transformation,\n",
    "] >> generate_user_behavior_metric >> end_of_data_pipeline\n",
    "```\n",
    "\n",
    "The sql query generates customer level aggregate metrics, using `spectrum.user_purchase_staging` and `spectrum.classified_movie_review`:\n",
    "```\n",
    "-- scripts/sql/generate_user_behavior_metric.sql\n",
    "\n",
    "DELETE FROM public.user_behavior_metric\n",
    "WHERE insert_date = '{{ ds }}';\n",
    "INSERT INTO public.user_behavior_metric (\n",
    "        customerid,\n",
    "        amount_spent,\n",
    "        review_score,\n",
    "        review_count,\n",
    "        insert_date\n",
    "    )\n",
    "SELECT ups.customerid,\n",
    "    CAST(\n",
    "        SUM(ups.Quantity * ups.UnitPrice) AS DECIMAL(18, 5)\n",
    "    ) AS amount_spent,\n",
    "    SUM(mrcs.positive_review) AS review_score,\n",
    "    count(mrcs.cid) AS review_count,\n",
    "    '{{ ds }}'\n",
    "FROM spectrum.user_purchase_staging ups\n",
    "    JOIN (\n",
    "        SELECT cid,\n",
    "            CASE\n",
    "                WHEN positive_review IS True THEN 1\n",
    "                ELSE 0\n",
    "            END AS positive_review\n",
    "        FROM spectrum.classified_movie_review\n",
    "        WHERE insert_date = '{{ ds }}'\n",
    "    ) mrcs ON ups.customerid = mrcs.cid\n",
    "WHERE ups.insert_date = '{{ ds }}'\n",
    "GROUP BY ups.customerid;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a90c9",
   "metadata": {},
   "source": [
    "## Airflow DAG\n",
    "\n",
    "This is the DAG diagram from Airflow after everything had been loaded and was ready to run:\n",
    "![Airflow DAG Diagram](https://lh3.googleusercontent.com/pw/AL9nZEX4hUXfVwPpPd6QKCKCMoNdzVzNJKrc2QtNJbbYwcErbTi7-mtms5cRyliIeiOBzQbXuAwG0aBwsb2A7DZKU8_-avv-gZrczLL9jcEo7LQ88sAiwQ84AWuQTMQ57Mwwj1KVg6jFloYbQvm108PvJUv-uA=w1909-h641-no?authuser=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3a6bf1",
   "metadata": {},
   "source": [
    "## Running the DAG and Verifying Activities\n",
    "\n",
    "This is the DAG after some running:\n",
    "![DAG in Airflow](https://lh3.googleusercontent.com/pw/AL9nZEX2Z6pJxFrUO7NmVZqmaJQUAKENSQU4sVBWIH7I0ZmgNMxkMp2xsZosXcqSu_FrIukx9XVkdAgYJ5-mgAHFNg8Or-fpvOOVS8FN_9SG8smroZ2av-AgxyKjUd5R4688BGMib1FJZb11q0XP1KacrkMVZg=w1560-h299-no?authuser=0)\n",
    "\n",
    "This is the EMR Cluster:\n",
    "![EMR Cluster](https://lh3.googleusercontent.com/pw/AL9nZEUh0XARIK6q17I63a0weGTf9PCdhXxtIGHOhU59dpc8-shmben3YtwFabCeNP-HaPoVR9OTaYc1oLzfUu7tJ_T_dC3GgqA2kKDFDcelmXr5Rno_tdRiQ8qM9UUeMBfknHzWWskxtnw2Yeiwatrnu4fwcA=w1560-h772-no?authuser=0)\n",
    "\n",
    "EMR Monitoring:\n",
    "![EMR Monitoring](https://lh3.googleusercontent.com/pw/AL9nZEVISjBSENotFLMRY_WjoeX348PRPegI7dXDVZZg4HSci5-qCcz9KlRddLuVFHn1mYF1zKdM-NatwQTEnLekFkb7FXJb0gHuIfH3YwpAIJWkNdflPeAPDKTFJD1WBWV3oKxVsGNXTpIZ10Kl9uu5nX0cxg=w1560-h770-no?authuser=0)\n",
    "\n",
    "The Redshift Cluster on AWS:\n",
    "![Redshift Cluster](https://lh3.googleusercontent.com/pw/AL9nZEV25msKSkgx7ohzuTavpmDnA7qzWeBDulPSA2tNQip34rWtzw7wNK80HqTObifjVdxW2NATrY-PrITX1ATO86L_6rGLX3AIyAWBGuQ0b5h-wuoiXcwSiWMR3ACpd0CuiDmxe6MqPxYw_bo-fAzAHf8sdw=w1560-h770-no?authuser=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c199abf",
   "metadata": {},
   "source": [
    "## Checking Results\n",
    "\n",
    "The `public.user_behavior_metric` Redshift table was checked as shown:\n",
    "```\n",
    "export REDSHIFT_HOST=$(aws redshift describe-clusters --cluster-identifier sde-batch-de-project --query 'Clusters[0].Endpoint.Address' --output text)\n",
    "psql postgres://sde_user:sdeP0ssword0987@$REDSHIFT_HOST:5439/dev\n",
    "```\n",
    "\n",
    "Using the SQL prompt, these queries were run to make sure the counts were as expected:\n",
    "```\n",
    "SELECT insert_date, count(*) AS cnt\n",
    "    FROM spectrum.classified_movie_review\n",
    "    GROUP BY insert_date\n",
    "    ORDER BY cnt desc; -- 100,000 per day\n",
    "\n",
    "SELECT insert_date, count(*) AS cnt\n",
    "    FROM spectrum.user_purchase_staging\n",
    "    GROUP BY insert_date\n",
    "    ORDER BY cnt desc; -- 541,908 per day\n",
    "\n",
    "SELECT insert_date, count(*) AS cnt\n",
    "    FROM public.user_behavior_metric\n",
    "    GROUP BY insert_date\n",
    "    ORDER BY cnt desc; -- 908 per day\n",
    "\n",
    "\\q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4dc8ab",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "Metabase was used to confirm the \n",
    "\n",
    "![Metabase Viz 1](https://lh3.googleusercontent.com/pw/AL9nZEUWyPxs71NkYJHVzWRacqpxf0jUeo9VDjJW25CjQb45MxWvYavEx4djzFxINwsvD9756rny5wzNynoBzTNyLwIB-Mi_DXqz_VdcTXWaM8y7OdoT_WgQ4vLen0KaLabf89O1WJ4pVXHG3Oc7D5yEw1ZzeA=w1560-h770-no?authuser=0)\n",
    "\n",
    "![Metabase Viz 2](https://lh3.googleusercontent.com/pw/AL9nZEVVie6ZTvt5_2W1lh0jJIhYdZY3D70WV9m8BpoeRJmZsLLz5fvqbr719gc0IGNxsrunUmHcCA35QGw_DMzlQUoiBGdj8wJH-lT2oiAHaaUZyliZXN5OSN6cqT4-sD4zBtG6CgRA9aBBnk02HkUOz9EcAQ=w1560-h770-no?authuser=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0116e",
   "metadata": {},
   "source": [
    "## Costs\n",
    "\n",
    "This project used:\n",
    "* 3 m4.xlarge type nodes for our AWS EMR cluster.\n",
    "* 1 dc2.large for our AWS Redshift cluster.\n",
    "* 1 iam role to allow Redshift access to S3.\n",
    "* 1 S3 bucket with about 150MB in size.\n",
    "* 1 t2.large AWS EC2 instance.\n",
    "\n",
    "By AWS estimates, the cost would be $313.91 per month, which is approximately 44 cents per hour:\n",
    "![AWS Costs](https://lh3.googleusercontent.com/pw/AL9nZEUgZ2ZaN750gS5HVYdubYUY1mAuOrWFz609eV0fGVobmrjwnjbsVH_S9sw8miB7nI5MSk5rqgZ7yXpRdU0mXXHITAKWx92nU81gp1GhD7Be6fiaoiiHPd0Bw047rxZGMAFfDLo7hqsISg_v_iJ3GJKDFQ=w1560-h618-no?authuser=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f1765b",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This project was a good introduction to all the components used in a data pipeline in order to load and aggregate data from different sources into one data warehouse for analysis.  It used a lot of AWS services that are common in data engineering and these tools could become very useful in many applications.\n",
    "\n",
    "After the project, we ran tear_down_infra.sh to shut down the AWS services and delete IAM users and security groups created specifically for this project.  There was some troubleshooting and debugging required to set everything up and some further exploration was done with monitoring and scheduling tasks on Airflow.\n",
    "\n",
    "Overall, it was good practice to get a pipeline up and running and see the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
